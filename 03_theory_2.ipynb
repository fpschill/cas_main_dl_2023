{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk-LGNwo9q08"
   },
   "source": [
    "# Deep Learning Discussions\n",
    "\n",
    "## MNIST Hello World (as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymzN_tJ2j_EP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotTraining(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='lower right')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bea1u4m49q1D"
   },
   "outputs": [],
   "source": [
    "# hello world example\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "522UyJtaLQ5J"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "- Differnt Types of Activation Functions. What do you obeserve? Training speed, final performance, etc...\n",
    "- Why do we need them at all?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whllDBRS9q1V"
   },
   "outputs": [],
   "source": [
    "# no activation function (linear)\n",
    "# https://keras.io/activations/\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  \n",
    "  #PLAY AROUND HERE\n",
    " \n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  #tf.keras.layers.Dense(64), #linear\n",
    "  #tf.keras.layers.Dense(64, activation=tf.nn.sigmoid),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niSWIKTBLQ5M"
   },
   "source": [
    "## Optimization - Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUFJvBTz9q1k"
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "# https://keras.io/initializers/\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  \n",
    "  #PLAY AROUND HERE\n",
    "  \n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  #tf.keras.layers.Dense(64, activation=tf.nn.relu,  kernel_initializer=tf.keras.initializers.Zeros()),\n",
    "  #tf.keras.layers.Dense(64, activation=tf.nn.relu,  kernel_initializer=tf.keras.initializers.Constant(value=-1)),\n",
    "  #tf.keras.layers.Dense(64, activation=tf.nn.relu,  kernel_initializer=tf.keras.initializers.Constant(value=1)),\n",
    "  \n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFju7L0fLQ5P"
   },
   "source": [
    "## Optimization - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GKQ32LB9q13"
   },
   "outputs": [],
   "source": [
    "# optimizer & stochastic gradient descent\n",
    "# https://keras.io/optimizers/\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "#PLAY AROUND HERE\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=1) #sgd\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=len(x_train)) #batch gradient descent\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64) #32 default\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHlaudXKw7dj"
   },
   "source": [
    "## Data Augmentation to increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vdtg1wzAj_Ek"
   },
   "outputs": [],
   "source": [
    "#no data augmentation\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Make train data smaller -- only 100 examples!\n",
    "np.random.seed(36)\n",
    "train_data_idx=np.random.choice(range(0,len(x_train)),100,replace=False)\n",
    "x_train=x_train[train_data_idx]\n",
    "y_train=y_train[train_data_idx]\n",
    "\n",
    "x_train=x_train.reshape((len(x_train),28,28,1))\n",
    "x_test=x_test.reshape((len(x_test),28,28,1))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28,1)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V19CHcvHjbzc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do data aug\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15)\n",
    "\n",
    "i=2\n",
    "data_aug=datagen.flow(x=x_train[i:(i+1)], y=y_train[i:(i+1)], batch_size=1)\n",
    "plt.imshow(x_train[i,:,:,0],cmap=\"gray\")\n",
    "# original image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6pLHKMej3rs"
   },
   "outputs": [],
   "source": [
    "# augmented image\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range (0,25):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  x_aug,y_aug=next(data_aug)\n",
    "  plt.imshow(x_aug[0,:,:,0],cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le8BmTSuj_Er"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Make train data smaller -- only 100 examples!\n",
    "np.random.seed(36)\n",
    "train_data_idx=np.random.choice(range(0,len(x_train)),100,replace=False)\n",
    "x_train=x_train[train_data_idx]\n",
    "y_train=y_train[train_data_idx]\n",
    "\n",
    "x_train=x_train.reshape((len(x_train),28,28,1))\n",
    "x_test=x_test.reshape((len(x_test),28,28,1))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32), epochs=300, steps_per_epoch=np.ceil(len(x_train)/32), validation_data=(x_test, y_test))\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDaS5HnAxZpw"
   },
   "source": [
    "### Accuracy on the test set is around 10% better if you use data augmentation\n",
    "\n",
    "0.7491 -> 0.8349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfdAS3Knj_Ev"
   },
   "source": [
    "## Other Architecture: CNN\n",
    "\n",
    "model from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQCsw8aOj_Ev"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, clone_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_digits_train, y_digits_train), (x_digits_test, y_digits_test) = mnist.load_data()\n",
    "\n",
    "# Make train data smaller\n",
    "np.random.seed(36)\n",
    "train_data_idx=np.random.choice(range(0,len(x_digits_train)),100,replace=False)\n",
    "x_digits_train=x_digits_train[train_data_idx]\n",
    "y_digits_train=y_digits_train[train_data_idx]\n",
    "\n",
    "# Preprocess data \n",
    "x_digits_train = x_digits_train.astype('float32')\n",
    "x_digits_test = x_digits_test.astype('float32')\n",
    "x_digits_train = x_digits_train/ 255\n",
    "x_digits_test = x_digits_test/ 255\n",
    "x_digits_train=x_digits_train.reshape((len(x_digits_train),28,28,1))\n",
    "x_digits_test=x_digits_test.reshape((len(x_digits_test),28,28,1))\n",
    "\n",
    "\n",
    "# Define model \n",
    "model_digits = Sequential()\n",
    "model_digits.add(Conv2D(8,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model_digits.add(Conv2D(8,(3,3),activation='relu'))\n",
    "model_digits.add(MaxPooling2D((2,2)))\n",
    "model_digits.add(Conv2D(16,(3,3),activation='relu'))\n",
    "model_digits.add(Conv2D(16,(3,3),activation='relu'))\n",
    "model_digits.add(Flatten())\n",
    "model_digits.add(Dense(50, activation='relu'))\n",
    "model_digits.add(Dense(10, activation='softmax'))\n",
    "# Compile model\n",
    "model_digits.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history=model_digits.fit(x_digits_train, y_digits_train,validation_data=(x_digits_test, y_digits_test),\n",
    "                         batch_size=128, epochs=80)\n",
    "plotTraining(history)\n",
    "\n",
    "model_digits.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN1zrkovj_Ex"
   },
   "source": [
    "## CNN + Data-Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqHjUjiVj_Ex"
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history=model_digits.fit(datagen.flow(x_digits_train, y_digits_train, batch_size=64),validation_data=(x_digits_test, y_digits_test),\n",
    "                    steps_per_epoch=len(x_digits_train) / 64, epochs=120)\n",
    "plotTraining(history)\n",
    "\n",
    "model_digits.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m3M1iQFj_E0"
   },
   "source": [
    "### Test performance summary (so far)\n",
    "\n",
    "- simple model:\n",
    " \n",
    "         0.7491 -> 0.8349\n",
    "    \n",
    "    \n",
    "- CNN (for other task, much much higher!):\n",
    "    \n",
    "        0.7685 -> 0.8414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Splt657fj_E1"
   },
   "source": [
    "## CNN + Fine-Tuning\n",
    "\n",
    "- train a model on letters (E-MNIST)\n",
    "- fix lower layers and fine-tune to digits (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJJy2JP9hwXe"
   },
   "outputs": [],
   "source": [
    "# Downloading the data, if it does not exist (takes some time)\n",
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "response = urllib.request.urlretrieve(\"http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip\", \"matlab.zip\")\n",
    "zf = zipfile.ZipFile(\"matlab.zip\")\n",
    "zf.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmx7Cf8mgHJY"
   },
   "outputs": [],
   "source": [
    "#load emnist data\n",
    "from scipy import io as spio\n",
    "\n",
    "emnist = spio.loadmat(\"matlab/emnist-letters.mat\")\n",
    "\n",
    "# load training dataset\n",
    "x_letter_train = emnist[\"dataset\"][0][0][0][0][0][0]\n",
    "y_letter_train = emnist[\"dataset\"][0][0][0][0][0][1]\n",
    "\n",
    "# load test dataset\n",
    "x_letter_test = emnist[\"dataset\"][0][0][1][0][0][0]\n",
    "y_letter_test = emnist[\"dataset\"][0][0][1][0][0][1]\n",
    "\n",
    "x_letter_train = x_letter_train.reshape(x_letter_train.shape[0], 28, 28, 1, order=\"A\")\n",
    "x_letter_test = x_letter_test.reshape(x_letter_test.shape[0], 28, 28, 1, order=\"A\")\n",
    "x_letter_train = x_letter_train.astype('float32')\n",
    "x_letter_test = x_letter_test.astype('float32')\n",
    "x_letter_train /= 255\n",
    "x_letter_test /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuJ3w3oAsYha"
   },
   "outputs": [],
   "source": [
    "# Define model \n",
    "model_letters = Sequential()\n",
    "model_letters.add(Conv2D(8,(3,3), activation='relu',input_shape=(28,28,1)))\n",
    "model_letters.add(Conv2D(8,(3,3), activation='relu'))\n",
    "model_letters.add(MaxPooling2D((2,2)))\n",
    "model_letters.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model_letters.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model_letters.add(MaxPooling2D((2,2)))\n",
    "model_letters.add(Flatten(name=\"Flat\"))\n",
    "model_letters.add(Dense(50, activation='relu'))\n",
    "model_letters.add(Dense(27, activation='softmax'))\n",
    "# Compile model\n",
    "model_letters.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history=model_letters.fit(x_letter_train, y_letter_train,validation_data=(x_letter_test, y_letter_test),\n",
    "                         batch_size=128, epochs=5, verbose=1)\n",
    "plotTraining(history)\n",
    "\n",
    "model_letters.evaluate(x_letter_test,y_letter_test)\n",
    "model_letters.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lNTdUNfjfhC"
   },
   "outputs": [],
   "source": [
    "letter_model = Model(inputs=model_letters.input, outputs=model_letters.get_layer(\"Flat\").output)\n",
    "letter_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9cqs9a0h1lV"
   },
   "outputs": [],
   "source": [
    "x = letter_model.output\n",
    "# add a hidden and the new output layer\n",
    "x = Dense(50, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=letter_model.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irtGhsEKkmwp"
   },
   "outputs": [],
   "source": [
    "#fix the lower layers\n",
    "for layer in letter_model.layers:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name,layer.trainable)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXu0onTTuCPE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "history=model.fit(x_digits_train, y_digits_train,validation_data=(x_digits_test, y_digits_test),\n",
    "                         batch_size=64, epochs=80)\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_TenR38yFB1"
   },
   "source": [
    "### Test performance summary (so far)\n",
    "\n",
    "- simple model:\n",
    " \n",
    "         0.7491 -> 0.8349 (data aug.)\n",
    "    \n",
    "    \n",
    "- CNN (for other task, much much higher!):\n",
    "    \n",
    "        0.7685 -> 0.8414 (data aug.)\n",
    "        0.7685 -> 0.8974 (fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjYlDyxf4K_6"
   },
   "source": [
    "##  CNN + Data augmentation and Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4pIq-iG4VYs"
   },
   "outputs": [],
   "source": [
    "letter_model = Model(inputs=model_letters.input, outputs=model_letters.get_layer(\"Flat\").output)\n",
    "\n",
    "x = letter_model.output\n",
    "x = Dense(50, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=letter_model.input, outputs=predictions)\n",
    "for layer in letter_model.layers:\n",
    "    layer.trainable = False   \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history=model.fit(datagen.flow(x_digits_train, y_digits_train, batch_size=64),validation_data=(x_digits_test, y_digits_test),\n",
    "                    steps_per_epoch=len(x_digits_train) / 64, epochs=120)\n",
    "plotTraining(history)\n",
    "\n",
    "model.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAkoO7i665kJ"
   },
   "source": [
    "### Test performance summary\n",
    "**trained ussing 100 images only!**\n",
    "\n",
    "- simple model:\n",
    " \n",
    "         0.7491 -> 0.8349 (data ug.)\n",
    "    \n",
    "- CNN (for other task, much much higher!):\n",
    "    \n",
    "        0.7685 -> 0.8414 (data aug.)\n",
    "        0.7685 -> 0.8974 (fine-tuning)\n",
    "        0.7685 -> 0.9251 (data aug. & fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSZAHpjzhWR"
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObUe8FSv0XFb"
   },
   "outputs": [],
   "source": [
    "## same network from scratch using all training data\n",
    "\n",
    "(x_digits_train, y_digits_train), (x_digits_test, y_digits_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Preprocess data \n",
    "x_digits_train = x_digits_train.astype('float32')\n",
    "x_digits_test = x_digits_test.astype('float32')\n",
    "x_digits_train = x_digits_train/ 255\n",
    "x_digits_test = x_digits_test/ 255\n",
    "x_digits_train=x_digits_train.reshape((len(x_digits_train),28,28,1))\n",
    "x_digits_test=x_digits_test.reshape((len(x_digits_test),28,28,1))\n",
    "\n",
    "\n",
    "# Define model \n",
    "model_digits_full = Sequential()\n",
    "model_digits_full.add(Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model_digits_full.add(Conv2D(16,(3,3),activation='relu'))\n",
    "model_digits_full.add(MaxPooling2D((2,2)))\n",
    "model_digits_full.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model_digits_full.add(Conv2D(32,(3,3),activation='relu'))\n",
    "\n",
    "model_digits_full.add(Flatten())\n",
    "model_digits_full.add(Dense(50, activation='relu'))\n",
    "model_digits_full.add(Dense(10, activation='softmax'))\n",
    "# Compile model\n",
    "model_digits_full.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history=model_digits_full.fit(x_digits_train, y_digits_train,validation_data=(x_digits_test, y_digits_test),\n",
    "                         batch_size=128, epochs=5, verbose=1)\n",
    "plotTraining(history)\n",
    "\n",
    "model_digits_full.evaluate(x_digits_test,y_digits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nukmu912wOg1"
   },
   "outputs": [],
   "source": [
    "wrong_idx=np.where(np.argmax(model_digits_full.predict(x_digits_test), axis=1)!=(y_digits_test))[0]\n",
    "len(wrong_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiVD575vzt5V"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "for i in range(0,50):\n",
    "  plt.subplot(10,5,i+1)\n",
    "  plt.imshow(x_digits_test[wrong_idx[i],:,:,0],cmap=\"gray\")\n",
    "  plt.title(\"pred: \"+str(np.argmax(model_digits_full.predict(x_digits_test[wrong_idx[i:(i+1)]]), axis=1)[0])+ \" true: \"+str((y_digits_test[wrong_idx[i]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02_theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
