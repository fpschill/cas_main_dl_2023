{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNvzsBhVypza",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simpsons characters classification \n",
    "\n",
    "In this notebook we try to classify images of different simpsons characters. The characters are 'abraham_grampa_simpson', 'apu_nahasapeemapetilon', 'bart_simpson', 'charles_montgomery_burns', 'chief_wiggum', 'homer_simpson', 'krusty_the_clown', 'lisa_simpson', 'marge_simpson', 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders', 'principal_skinner' and 'sideshow_bob'.\n",
    "This dataset was preprocessed in an other notebook, it is splitted into a train val and testset and resized into 80x80 pixels and all characters have more than 600 images in total. The whole dataset with the original size can be found here https://www.kaggle.com/alexattia/the-simpsons-characters-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCCwPN1J7iqK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLeRfAvImGyK",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBNSW2X-mGyT",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj3NQ0j1vyzb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  Google Colab: Import Data from Google Drive to Colab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ge27BT_4vEV"
   },
   "source": [
    "Open this link in your browser: https://drive.google.com/drive/folders/1ymYg4DUj1ft-kUbuU9rW0bNHAttW44Zv?usp=sharing, then:\n",
    "\n",
    "Option 1:\n",
    "\n",
    "- download data.zip from the shared folder above to your local hard disk\n",
    "- then copy it into your google drive\n",
    "\n",
    "Option 2:\n",
    "\n",
    "- in the above shared folder, open drop down in folder name \"CAS_MAIN_DL\"\n",
    "- select \"add short cut to drive\"\n",
    "\n",
    "Depending on option 1 or 2, adapt the \"path\" variable two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8iQqzBg2Yi_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os,zipfile,io\n",
    "\n",
    "# mount your google drive as /content/drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# path to the data.zip file within your google drive (option 1 above)\n",
    "# my own file\n",
    "#ddir=\"/content/drive/MyDrive/Shared/ZHAW/CAS_MAIN_DL\"\n",
    "\n",
    "# shared with me, then select \"add shortcut to drive\" (option 2 above)\n",
    "ddir=\"/content/drive/MyDrive/CAS_MAIN_DL\"\n",
    "\n",
    "zf = zipfile.ZipFile(os.path.join(ddir,\"data.zip\"), \"r\")\n",
    "zf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWWaPITIG-IW"
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zmmohbIyd8l",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./data/simpson_data\"\n",
    "Data = pd.read_csv(os.path.join(path, \"Data.csv\"))\n",
    "X_train = np.load(os.path.join(path, \"X_train.npy\"))\n",
    "Y_train = np.load(os.path.join(path, \"Y_train.npy\"))\n",
    "X_val = np.load(os.path.join(path, \"X_val.npy\"))\n",
    "Y_val = np.load(os.path.join(path, \"Y_val.npy\"))\n",
    "X_test = np.load(os.path.join(path, \"X_test.npy\"))\n",
    "Y_test = np.load(os.path.join(path, \"Y_test.npy\"))\n",
    "labels = Data[\"label\"].unique()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sUVgw_G7oN4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's use the trainset to plot a random image of each character. You can see that the characters are easy recognizable. And all images are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVMzs5EJyd8u",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n",
    "    rmd = np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n",
    "    plt.subplot(4,4,i+1)\n",
    "    img = X_train[rmd]\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.title(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8p2GxmZ8H3z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this cell we plot the label distribution of all sets. You clearly see that the label distribution in all sets is very similar. The biggest class in the trainigset is obviously homer and the smallest class is apu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BO-kqbdcCkM6",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(np.unique(np.argmax(Y_train,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"train distribution\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.unique(np.argmax(Y_val,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_val,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"val distribution\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(np.unique(np.argmax(Y_test,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_test,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"test distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkq49jpMQcW-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CNN\n",
    "\n",
    "Now we normalize the data and use a CNN to classify the images into the right simpson character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcHL2TwAImOB",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train,dtype=\"float32\")\n",
    "X_train=((X_train/255)-0.5)*2\n",
    "\n",
    "X_val=np.array(X_val,dtype=\"float32\")\n",
    "X_val=((X_val/255)-0.5)*2\n",
    "\n",
    "X_test=np.array(X_test,dtype=\"float32\")\n",
    "X_test=((X_test/255)-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLmaVt_uG9ds",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model  =  Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",padding=\"same\",input_shape=(80,80,3)))\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(14))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqJ5C7AdLm75",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode=\"constant\",\n",
    "    cval=255,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keJSvIrVMFyI",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=22\n",
    "data_aug = datagen.flow(x=X_train[i:(i+1)], y=Y_train[i:(i+1)], batch_size=1)\n",
    "print(np.min(X_train[i]),np.max(X_train[i]))\n",
    "plt.imshow(X_train[i])\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range (0,25):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  x_aug,y_aug=next(data_aug)\n",
    "  plt.imshow(x_aug[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2Vrbz-9Mpv7",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(datagen.flow(X_train, Y_train, batch_size=32), \n",
    "                              steps_per_epoch=len(X_train)/32, \n",
    "                              epochs=150, # 150 \n",
    "                              validation_data=(X_val, Y_val),\n",
    "                              verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ki1VpnthJx2",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-8sSFFFm-iH",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./data/simpson_data/simpsons_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBLxYA4H8fG7",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load reference model\n",
    "# model=load_model(\"./data/simpson_data/simpsons_cnn_ref.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWvnpdSnBRyZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate\n",
    "Lets check the overall accuracy and the accuracy per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqHyB9irnHIM",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(model.predict(X_test),axis=1) == np.argmax(Y_test,axis=1))\n",
    "res = pd.DataFrame({'Acc' : acc}, index=['CNN'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyH6Nj0QFZ2R",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(labels)):\n",
    "  print(labels[i],np.average(np.argmax(model.predict(X_test),axis=1)[np.where(np.argmax(Y_test,axis=1)==i)]==i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMxxxoRuFteW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let us do a method called test time augmentation, where we predict the same image over multiple different augmentation runs. For a final prediction for that image we average over all the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QvFR_dcXobc",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aug_size = 40\n",
    "tta_pred = np.zeros((len(X_test),14))\n",
    "for i in tqdm(range(0,len(X_test))):\n",
    "  tmp = np.zeros((aug_size,80,80,3))\n",
    "  data_aug = datagen.flow(x=X_test [i:(i+1)], y=Y_test[i:(i+1)], batch_size=1)\n",
    "  for j in range(0,aug_size):\n",
    "    tmp[j],_ = next(data_aug)\n",
    "  tta_pred[i] = np.average(model.predict(tmp),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA19zzt682EO",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = np.average(np.argmax(tta_pred,axis=1)==np.argmax(Y_test,axis=1))\n",
    "res = pd.DataFrame({'Acc' : acc}, index=['CNN_tta'])\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sThxVwhtCkNL",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(labels)):\n",
    "  print(labels[i],np.average(np.argmax(tta_pred,axis=1)[np.where(np.argmax(Y_test,axis=1)==i)]==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1LD_BNuCkNL",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "04_3_simpsons_classification_cnn_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
