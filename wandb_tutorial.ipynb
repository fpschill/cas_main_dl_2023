{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5OjOERtAUta"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{intro-colab-keras-metricslogger} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBNpTjXAUtc"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{intro-colab-keras-metricslogger} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCWcHBYYAUtd"
      },
      "source": [
        "\n",
        "# Using Keras MetricsLogger in your Keras workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNGMfiYAUte"
      },
      "source": [
        "\n",
        "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
        "\n",
        "<img src=\"http://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VjdMgGbAUte"
      },
      "source": [
        "This colab notebook introduces the `WandbMetricsLogger` callback. Use this callback for [Experiment Tracking](https://docs.wandb.ai/guides/track). It will log your training and validation metrics along with system metrics to Weights and Biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHtI-0bZAUtf"
      },
      "source": [
        "# üå¥ Setup and Installation\n",
        "\n",
        "First, let us install the latest version of Weights and Biases. We will then authenticate this colab instance to use W&B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqqFYW6KAUtf"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKEaWcQ-AUtg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Weights and Biases related imports\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmLxwDkyAUth"
      },
      "source": [
        "If this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login()` will take you to sign-up/login page. Signing up for a [free account](https://wandb.ai/signup) is as easy as a few clicks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGDwuY-FAUth"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw1qS-FAUth"
      },
      "source": [
        "# üå≥ Hyperparameters\n",
        "\n",
        "Use of proper config system is a recommended best practice for reproducible machine learning. We can track the hyperparameters for every experiment using W&B. In this colab we will be using simple Python `dict` as our config system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WYg6ASHAUti"
      },
      "outputs": [],
      "source": [
        "configs = dict(\n",
        "    num_classes = 10,\n",
        "    shuffle_buffer = 1024,\n",
        "    batch_size = 64,\n",
        "    image_size = 28,\n",
        "    image_channels = 1,\n",
        "    earlystopping_patience = 3,\n",
        "    learning_rate = 1e-3,\n",
        "    epochs = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYkHMWZtAUti"
      },
      "source": [
        "# üçÅ Dataset\n",
        "\n",
        "In this colab, we will be using [CIFAR100](https://www.tensorflow.org/datasets/catalog/cifar100) dataset from TensorFlow Dataset catalog. We aim to build a simple image classification pipeline using TensorFlow/Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRv0K9GUAUtj"
      },
      "outputs": [],
      "source": [
        "train_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnDAHds5AUtk"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def parse_data(example):\n",
        "    # Get image\n",
        "    image = example[\"image\"]\n",
        "    # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "\n",
        "    # Get label\n",
        "    label = example[\"label\"]\n",
        "    label = tf.one_hot(label, depth=configs[\"num_classes\"])\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def get_dataloader(ds, configs, dataloader_type=\"train\"):\n",
        "    dataloader = ds.map(parse_data, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if dataloader_type==\"train\":\n",
        "        dataloader = dataloader.shuffle(configs[\"shuffle_buffer\"])\n",
        "\n",
        "    dataloader = (\n",
        "        dataloader\n",
        "        .batch(configs[\"batch_size\"])\n",
        "        .prefetch(AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vPKlz8HAUtk"
      },
      "outputs": [],
      "source": [
        "trainloader = get_dataloader(train_ds, configs)\n",
        "validloader = get_dataloader(valid_ds, configs, dataloader_type=\"valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4EbhZPDAUtk"
      },
      "source": [
        "# üéÑ Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GUmqmHtAUtk"
      },
      "outputs": [],
      "source": [
        "def get_model(configs):\n",
        "    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False)\n",
        "    backbone.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=(configs[\"image_size\"], configs[\"image_size\"], configs[\"image_channels\"]))\n",
        "    resize = layers.Resizing(32, 32)(inputs)\n",
        "    neck = layers.Conv2D(3, (3,3), padding=\"same\")(resize)\n",
        "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input(neck)\n",
        "    x = backbone(preprocess_input)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(configs[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    return models.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRq1FzgiAUtl"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = get_model(configs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUluJSdVAUtl"
      },
      "source": [
        "# üåø Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yzL-2zCAUtl"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top@5_accuracy')]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ROe5NUAUtl"
      },
      "source": [
        "# üåª Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0W8M5pcAUtl"
      },
      "outputs": [],
      "source": [
        "# Initialize a W&B run\n",
        "run = wandb.init(\n",
        "    project = \"intro-keras\",\n",
        "    config = configs\n",
        ")\n",
        "\n",
        "# Train your model\n",
        "model.fit(\n",
        "    trainloader,\n",
        "    epochs = configs[\"epochs\"],\n",
        "    validation_data = validloader,\n",
        "    callbacks = [WandbMetricsLogger(log_freq=10)] # Notice the use of WandbMetricsLogger here\n",
        ")\n",
        "\n",
        "# Close the W&B run\n",
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}